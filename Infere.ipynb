{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPM7-E8mFFmj",
        "outputId": "8e3a1ed8-189f-44fc-c8a6-9937de7eb6f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-weQlmbEKO-U",
        "outputId": "39146cf3-1301-4911-b197-65d361c0cbd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "3CK8ZQQPEvmD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "VFiuGbpgF35l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cC_IKzhLKZEd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=\"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\")"
      ],
      "metadata": {
        "id": "Ze_-M5soMB9Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_case_outcome(case_text, client):\n",
        "    prompt = f\"\"\"System: You are a precise legal predictor with specialized training in court case outcome analysis. Accuracy is critical. The user will provide a legal case text, and you must predict who wins.\n",
        "\n",
        "User: Analyze this legal case and predict the outcome:\n",
        "\n",
        "{case_text}\n",
        "\n",
        "Follow this structured analysis approach:\n",
        "1. First, identify the exact legal question(s) before the court\n",
        "2. List key facts that directly impact the legal determination\n",
        "3. Examine controlling statutes and precedent cases mentioned\n",
        "4. Compare the strength of each party's arguments and evidence\n",
        "5. Consider procedural and jurisdictional factors that may influence the outcome\n",
        "\n",
        "After your analysis, predict the outcome by stating ONLY a single digit:\n",
        "0 = Plaintiff wins\n",
        "1 = Defendant wins\n",
        "\n",
        "Your response should contain ONLY the digit 0 or 1, with no explanation or additional text.\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal reasoning expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=5\n",
        "        )\n",
        "        output = response.choices[0].message.content.strip()\n",
        "\n",
        "        if \"0\" in output:\n",
        "            return 0\n",
        "        elif \"1\" in output:\n",
        "            return 1\n",
        "        else:\n",
        "            print(f\"Warning: Unexpected model response: {output}. Defaulting to 1.\")\n",
        "            return 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in prediction: {str(e)}\")\n",
        "        return 1\n",
        "\n",
        "# 3. Function to process cases from CSV\n",
        "def process_cases(input_file, output_file):\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Please upload the {input_file} file\")\n",
        "        uploaded = files.upload()\n",
        "        if input_file not in uploaded:\n",
        "            raise ValueError(f\"Failed to upload {input_file}\")\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    if 'id' not in df.columns or 'text' not in df.columns:\n",
        "        raise ValueError(\"Input CSV must contain 'id' and 'text' columns\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing cases\"):\n",
        "        case_id = row['id']\n",
        "        case_text = row['text']\n",
        "\n",
        "        outcome = predict_case_outcome(case_text, client)\n",
        "        results.append({'id': case_id, 'value': outcome})\n",
        "\n",
        "    output_df = pd.DataFrame(results)\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Predictions saved to {output_file}\")"
      ],
      "metadata": {
        "id": "j4H4FQl1E5h-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the original CSV file\n",
        "input_file = 'cases.csv'\n",
        "output_file = 'cases1.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "df_subset = df.head(50)\n",
        "df_subset.to_csv(output_file, index=False)\n",
        "print(f\"First 50 entries saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWINiRVBOk1J",
        "outputId": "2f3eff37-7051-46e1-c45d-04ecb9cf5a88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 entries saved to cases1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  input_csv = \"/content/cases1.csv\"\n",
        "  output_csv = \"/content/output3.csv\"\n",
        "  process_cases(input_csv, output_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u5hDzEnqFoX4",
        "outputId": "55d6ac67-7360-4fa4-c72b-106f5792f085"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  20%|██        | 10/50 [03:21<10:07, 15.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 18173, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  50%|█████     | 25/50 [07:19<03:42,  8.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98722, Requested 2390. Please try again in 16m0.623999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98722, Requested 10548. Please try again in 2h13m29.001999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing cases:  52%|█████▏    | 26/50 [07:19<02:30,  6.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98722, Requested 7716. Please try again in 1h32m42.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  58%|█████▊    | 29/50 [07:23<00:57,  2.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99576, Requested 1973. Please try again in 22m18.208999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99576, Requested 18174. Please try again in 4h15m35.726999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  62%|██████▏   | 31/50 [07:23<00:26,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99576, Requested 9242. Please try again in 2h6m58.358999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99576, Requested 9000. Please try again in 2h3m29.094999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  66%|██████▌   | 33/50 [07:23<00:12,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99576, Requested 2242. Please try again in 26m10.054999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99576, Requested 5192. Please try again in 1h8m38.726s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing cases:  68%|██████▊   | 34/50 [07:24<00:09,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99575, Requested 2787. Please try again in 34m0.664s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  72%|███████▏  | 36/50 [07:24<00:05,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99575, Requested 32740. Please try again in 7h45m19.743s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99575, Requested 3338. Please try again in 41m56.283s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  76%|███████▌  | 38/50 [07:24<00:03,  3.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99575, Requested 9752. Please try again in 2h14m17.864s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99575, Requested 1661. Please try again in 17m47.114s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  80%|████████  | 40/50 [07:25<00:01,  5.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99574, Requested 6205. Please try again in 1h23m13.008s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99574, Requested 2029. Please try again in 23m4.803999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing cases:  82%|████████▏ | 41/50 [07:25<00:01,  5.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99574, Requested 3349. Please try again in 42m5.162s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  86%|████████▌ | 43/50 [07:25<00:01,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99574, Requested 13853. Please try again in 3h13m20.394s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99574, Requested 12821. Please try again in 2h58m28.619999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  90%|█████████ | 45/50 [07:25<00:00,  6.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99574, Requested 5180. Please try again in 1h8m26.669s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 7589. Please try again in 1h43m7.927s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  94%|█████████▍| 47/50 [07:26<00:00,  7.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 6768. Please try again in 1h31m18.458s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 2337. Please try again in 27m29.957999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases:  98%|█████████▊| 49/50 [07:26<00:00,  7.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 1109. Please try again in 9m48.831s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 4345. Please try again in 56m24.620999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases: 100%|██████████| 50/50 [07:26<00:00,  8.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in prediction: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7pf3ygfghvttvdrxb3gfww` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 5946. Please try again in 1h19m27.764999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "✅ Predictions saved to /content/output3.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}